{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rake_nltk import Rake\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"data.csv\",encoding='iso-8859-1')\n",
    "data.dropna(subset=['Basic Rights'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Function to extract keywords out of a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extractor(query):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(query)\n",
    "    keywords = r.get_ranked_phrases()\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Using google translator for supporting different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Taking the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell what to do if police arrest me without reason\n",
      "Tell what to do if police arrest me without reason\n"
     ]
    }
   ],
   "source": [
    "user_query=input()\n",
    "print(user_query)\n",
    "user_query=translator.translate(user_query, src='hi', dest='en').text  #source=hindi destination=english\n",
    "print(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Processing the user query and extracting keywords from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['without', 'reason', 'police', 'arrest', 'tell']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_keywords = keyword_extractor(user_query)\n",
    "user_que = []\n",
    "for phrase in user_keywords:\n",
    "    words = phrase.split()\n",
    "    user_que.extend(words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "user_word = [lemmatizer.lemmatize(word) for word in user_que]\n",
    "\n",
    "common_words=[\"give\",\"information\",\"law\",\"provide\",\"know\",\"let\",\"regarding\",\"info\"]\n",
    "user_words = [word for word in user_word if word not in common_words]\n",
    "user_words = [string.lower() for string in user_words]\n",
    "user_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Finding the best matching law from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Protection against arrest and detention in certain cases',\n",
       " 'Investigation and Arrest']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_matching =[]\n",
    "max_matching_keywords = 0\n",
    "for law in list(data['Basic Rights']):\n",
    "    temp_list=law.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    right = [lemmatizer.lemmatize(word) for word in temp_list]\n",
    "    right = [string.lower() for string in right]\n",
    "    matching_keywords = len(set(user_words) & set(right))\n",
    "    best_matching.append([matching_keywords,law])\n",
    "\n",
    "best_matching_law=[]\n",
    "\n",
    "best_match=sorted(best_matching,reverse=True)\n",
    "thr=max(best_match)[0]\n",
    "for i in range(len(best_match)):\n",
    "    if best_match[i][0]==thr:\n",
    "        best_matching_law.append(best_match[i][1])\n",
    "best_matching_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Converting the dataframe to dictionary to process the best matching law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = data.set_index('Basic Rights').to_dict()['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "# import nltk\n",
    "# model_name = \"bert-base-uncased\"\n",
    "\n",
    "# # a) Get predictions\n",
    "# nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "\n",
    "# def get_answer(question, context):\n",
    "#     QA_input = {\n",
    "#         'question': question,\n",
    "#         'context': context\n",
    "#     }\n",
    "#     res = nlp(QA_input)\n",
    "#     return res['answer']\n",
    "\n",
    "# answers_list = []    \n",
    "\n",
    "# for i in range(len(best_matching_law)):\n",
    "#     result = get_answer( user_query,result_dict[best_matching_law[i]])\n",
    "#     answers_list.append(result)\n",
    "# answers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Using a pretrained model (Bert-Bidirectional Encoder Representations from Transformers) which could gemerate response according to the query and the context to the law\n",
    "    Context: The description from the dataset about the best matching law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "def answer_question(question, context):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    start_scores, end_scores = model(**inputs).values()\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
    "    answer = tokenizer.convert_tokens_to_string(all_tokens[answer_start:answer_end+1])\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Handling the query and generating the response in english and his regional language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1 कोई भी व्यक्ति जिसे गिरफ्तार नहीं किया जाता है उसे हिरासत में नहीं लिया जाएगा, बिना सूचित किए, जैसे ही हो सकता है, इस तरह की गिरफ्तारी के लिए आधार का\n",
      "Answer: 1 no person who is arrested shall be detained in custody without being informed , as soon as may be , of the grounds for such arrest\n",
      "Answer: 2 यदि किसी अपराध में किसी व्यक्ति की भागीदारी का समर्थन करने के लिए पर्याप्त सबूत हैं, तो उन्हें गिरफ्तार किया जा सकता है, कुछ कानूनी प्रक्रियाओं और सुरक्षा उपायों के अधीन\n",
      "Answer: 2 if there is sufficient evidence to support the involvement of an individual in a crime , they may be arrested , subject to certain legal procedures and safeguards\n"
     ]
    }
   ],
   "source": [
    "answers_list=[]\n",
    "handle_query=\"I'm sorry, but I don't have access to your query as of my knowledge.To find out answer to your query, I recommend checking the latest news.\"\n",
    "if (len(best_matching_law)!=0):\n",
    "    for i in range(len(best_matching_law)):\n",
    "        result = answer_question(user_query,result_dict[best_matching_law[i]])\n",
    "        answers_list.append(result)\n",
    "    for i in range(len(answers_list)):\n",
    "        temp=translator.translate(answers_list[i], src='en', dest='hi').text\n",
    "        print(\"Answer:\",i+1,temp)\n",
    "        print(\"Answer:\",i+1,answers_list[i])\n",
    "else:\n",
    "    print(handle_query)\n",
    "    print(translator.translate(handle_query, src='en', dest='hi').text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
